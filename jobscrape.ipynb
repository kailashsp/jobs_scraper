{"cells":[{"cell_type":"markdown","metadata":{},"source":["##  LINKEDIN ##"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import re\n","import cloudscraper\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","import os\n","import glob"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["scraper = cloudscraper.create_scraper()"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def extract(page):\n","    url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/search?keywords=Web%20Development&location=India&geoId=102713980&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0&start={page}\"\n","    \n","    r = scraper.get(url=url,headers={'User-Agent' :'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrohttpsme/83.0.4103.97 Safari/537.36'})\n","    soup = BeautifulSoup(r.content,'html.parser')\n","    return soup"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def transform(soup):\n","    divs = soup.find_all('div',class_='base-card relative w-full hover:no-underline focus:no-underline base-card--link base-search-card base-search-card--link job-search-card')\n","    for item in divs:\n","        title = item.find('h3','base-search-card__title').text.strip()\n","        company = item.find('h4','base-search-card__subtitle').text.strip()\n","        location = item.find('span','job-search-card__location').text.strip()\n","\n","        # posted = item.find('time',class_ = 'job-search-card__listdate').text.strip()\n","        posted = item.time.attrs['datetime']\n","        link = item.find('a', class_='base-card__full-link absolute top-0 right-0 bottom-0 left-0 p-0 z-[2]')['href']+'/'\n","        job = {\n","            'Title':title,\n","            'Company':company,\n","            'Location':location,\n","            # 'posted_on':posted,\n","            'Links':link\n","        }\n","        joblist.append(job)\n","    return"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["joblist = []"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["getting page 0\n","getting page 1\n","getting page 2\n","getting page 3\n","getting page 4\n","getting page 5\n","getting page 6\n","getting page 7\n","getting page 8\n","getting page 9\n","getting page 10\n","getting page 11\n","getting page 12\n","getting page 13\n","getting page 14\n","getting page 15\n","getting page 16\n","getting page 17\n","getting page 18\n","getting page 19\n","getting page 20\n","getting page 21\n","getting page 22\n","getting page 23\n","getting page 24\n","getting page 25\n","getting page 26\n","getting page 27\n","getting page 28\n","getting page 29\n","getting page 30\n","getting page 31\n","getting page 32\n","getting page 33\n","getting page 34\n","getting page 35\n","getting page 36\n","getting page 37\n","getting page 38\n","getting page 39\n"]}],"source":["for i in range(0,40): \n","    print(f'getting page {i}')                   #looping through the different pages and applying extract and tranform function\n","    \n","    c = extract(i)\n","    transform(c)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(joblist)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["d1 = df.to_csv('linkedin_jobs.csv',index=False)"]},{"cell_type":"markdown","metadata":{},"source":["## INTERSHALA ##"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def extract(page):\n","    '''function fetches content from web a page and\n","    return the content as soup object '''\n","    \n","    url =f\"https://internshala.com/jobs/web-development-jobs/page-{page}/\"\n","    r = scraper.get(url,headers={'User-Agent' :'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrohttpsme/83.0.4103.97 Safari/537.36'})\n","    soup = BeautifulSoup(r.content,'html.parser')\n","    return soup"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def transform(soup):\n","    '''manipulates the soup object and gets the details of the job'''\n","    divs = soup.find_all('div',class_='internship_meta')\n","    for item in divs:\n","        title = item.find('a').text\n","        company = item.find('div',class_ = 'heading_6 company_name').text.strip()\n","        location = item.find('a',class_ = 'location_link view_detail_button').text.strip()\n","        # salary = item.find_all('div',class_ = 'item_body')[1].text.strip()\n","        links = 'https://www.internshala.com'+item.find(\"a\",{\"class\":\"view_detail_button\"}).get(\"href\")+'/'\n","        # posted = item.find('div')\n","        # link = item.find_all(href = re.compile(\"^https://\"))\n","        # print(link)\n","        \n","        job = {\n","            'Title':title,\n","            'Company':company,\n","            'Location': location,\n","            # 'Salary':salary,\n","            'Links':links\n","        }\n","        joblist.append(job)\n","    return"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["joblist = []"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["getting page {0}\n","getting page {1}\n","getting page {2}\n"]}],"source":["for i in range(3):                      #looping through the different pages and applying extract and tranform function\n","    print(f'getting page',{i})\n","    c = extract(i)\n","    transform(c)"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["df = pd.DataFrame(joblist)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["d2 = df.to_csv('internshala_jobs.csv',index= False)"]},{"cell_type":"markdown","metadata":{},"source":["## MONSTER ##"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["scraper = cloudscraper.create_scraper()\n","def extract(page):\n","    '''function fetches content from web a page and\n","    return the content as soup object '''\n","\n","    url =f\"https://www.monsterindia.com/search/web-development-front-end-development-backend-developer-jobs-in-india-{page}?searchId=d10b0203-c330-41c4-acb1-34a6e7dd3711\"\n","    r = scraper.get(url,headers={'User-Agent' :'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrohttpsme/83.0.4103.97 Safari/537.36'})\n","    soup = BeautifulSoup(r.content,'html.parser')\n","    return soup\n","\n"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["\n","def transform(soup):\n","    '''manipulates the soup object and gets the details of the job'''\n","    divs = soup.find_all('div',class_='card-apply-content')\n","    for item in divs:\n","        title = item.find('a').text\n","        company = item.find('span',class_ = 'company-name').text.strip()\n","        summary = item.find('p',class_ = 'job-descrip').text.strip()\n","        location = item.find('small').text.strip()\n","        links = item.find_all(\"a\")[0].get(\"href\")+'/'\n","\n","        job = {\n","            'Title':title,\n","            'Company':company,\n","            'Location':location,\n","            # 'Summary': summary,\n","            'Links':links\n","        }\n","        joblist.append(job)\n","\n","    return \n"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["getting page {0}\n","getting page {1}\n","getting page {2}\n","getting page {3}\n","getting page {4}\n","getting page {5}\n","getting page {6}\n","getting page {7}\n","getting page {8}\n","getting page {9}\n"]}],"source":["\n","joblist = []\n","\n","\n","for i in range(10):                      #looping through the different pages and applying extract and tranform function\n","    print(f'getting page',{i})\n","    c = extract(i)\n","    transform(c)\n","\n","\n","\n","df = pd.DataFrame(joblist)\n","\n","df.to_csv('monster_jobs.csv')\n"]},{"cell_type":"markdown","metadata":{},"source":["## COMBINED CSV ##"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[],"source":["joined_files = os.path.join(os.curdir, \"*.csv\")\n","joined_list = glob.glob(joined_files)\n","d3 = pd.concat(map(pd.read_csv, joined_list), ignore_index=True,axis=0)\n","d4 = d3.to_csv('jobs.csv',index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3.9.12 ('av')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"440a55496de0bf34155db44885f997983cfd7ceae9392a3b306a75b145d42614"}}},"nbformat":4,"nbformat_minor":2}
